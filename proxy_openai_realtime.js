// npm install ws axios fs
import WebSocket, { WebSocketServer } from "ws";
import axios from "axios";
import fs from "fs";

const PORT = process.env.PORT || 10000;
const OPENAI_KEY = process.env.OPENAI_API_KEY;

if (!OPENAI_KEY) throw new Error("OPENAI_API_KEY not set");

//
// === 1. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Realtime-ÑĞµÑÑĞ¸Ğ¸ ===
//
async function createRealtimeSession() {
  const response = await axios.post(
    "https://api.openai.com/v1/realtime/sessions",
    {
      model: "gpt-4o-realtime-preview-2024-12-17",
    },
    {
      headers: {
        Authorization: `Bearer ${OPENAI_KEY}`,
        "Content-Type": "application/json",
      },
    }
  );
  return response.data;
}

//
// === 2. Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ WebSocket-ÑĞµÑ€Ğ²ĞµÑ€Ğ° ===
//
async function start() {
  console.log(`\nğŸš€ Proxy listening on ws://0.0.0.0:${PORT}`);
  if (process.env.RENDER_SERVICE_NAME) {
    console.log(
      `   WebSocket URL: wss://${process.env.RENDER_SERVICE_NAME}.onrender.com/ws`
    );
  }

  const wss = new WebSocketServer({ port: PORT, path: "/ws" });

  wss.on("connection", async (esp) => {
    console.log("âœ… ESP connected");
    console.log("ESP IP:", esp._socket.remoteAddress);

    try {
      //
      // === 3. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Realtime-ÑĞµÑÑĞ¸Ñ ===
      //
      const session = await createRealtimeSession();
      const clientSecret =
        session?.client_secret?.value || session?.client_secret;
      if (!clientSecret)
        throw new Error("No client_secret in OpenAI response");

      const wsUrl = `wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17&client_secret=${encodeURIComponent(
        clientSecret
      )}`;

      const oa = new WebSocket(wsUrl, {
        headers: {
          Authorization: `Bearer ${clientSecret}`,
          "OpenAI-Beta": "realtime=v1",
        },
      });

      //
      // === 4. ĞŸĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ===
      //
      let ready = false;
      let pendingChunks = [];
      let audioBuffer = [];
      let flushTimer = null;
      const FLUSH_THRESHOLD = 8;
      const FLUSH_INTERVAL = 200;

      //
      // === 5. Ğ‘ÑƒÑ„ĞµÑ€Ğ½Ğ°Ñ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ° ===
      //
      function flushAudioBuffer() {
        if (audioBuffer.length === 0 || oa.readyState !== WebSocket.OPEN)
          return;

        const full = Buffer.concat(audioBuffer);
        const base64 = full.toString("base64");

        // Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² Ñ„Ğ°Ğ¹Ğ» Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸
        fs.appendFileSync("session_audio.raw", full);

        oa.send(
          JSON.stringify({
            type: "input_audio_buffer.append",
            audio: base64,
          })
        );

        console.log(`ğŸ“¤ Sent batch: ${audioBuffer.length} chunks (${full.length} bytes)`);
        audioBuffer = [];

        clearTimeout(flushTimer);
        flushTimer = null;
      }

      //
      // === 6. ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ OpenAI ===
      //
      oa.on("open", () => {
        console.log("ğŸ”— Connected to OpenAI Realtime");
        ready = true;
      });

      oa.on("message", (data) => {
        const msg = data.toString();
        try {
          const parsed = JSON.parse(msg);

          if (parsed.type === "session.created") {
            console.log("ğŸŸ¢ OpenAI session ready");
            ready = true;
          }

          if (parsed.type === "response.audio_transcript.done") {
            console.log("ğŸ—£ Transcript:", parsed.transcript);
          }

          if (parsed.type === "response.error") {
            console.error("âŒ Response error:", parsed.error);
          }

          if (parsed.type === "error") {
            console.error("âŒ OpenAI Error:", parsed.error);
          }
        } catch (err) {
          console.error("âš ï¸ Parse error:", err.message);
        }
      });

      oa.on("close", () => console.log("ğŸ”Œ OpenAI closed"));
      oa.on("error", (e) =>
        console.error("âŒ OpenAI WS Error:", e.message)
      );

      //
      // === 7. ĞŸÑ€Ğ¸Ñ‘Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚ ESP ===
      //
      esp.on("message", (msg) => {
        if (Buffer.isBuffer(msg)) {
          console.log(`ğŸ§ Got ${msg.length} bytes from ESP`);
          audioBuffer.push(msg);
          if (audioBuffer.length >= FLUSH_THRESHOLD) {
            flushAudioBuffer();
          } else {
            clearTimeout(flushTimer);
            flushTimer = setTimeout(flushAudioBuffer, FLUSH_INTERVAL);
          }
          return;
        }

        const text = msg.toString().trim();

        if (text.includes("STREAM STOPPED")) {
          console.log("ğŸ›‘ Stream stopped â€” committing buffer");
          flushAudioBuffer();

          setTimeout(() => {
            oa.send(JSON.stringify({ type: "input_audio_buffer.commit" }));
            oa.send(
              JSON.stringify({
                type: "response.create",
                response: {
                  modalities: ["text"],
                  instructions:
                    "Return only the raw transcription of the spoken audio.",
                },
              })
            );
            console.log("ğŸ“¨ Sent commit + response.create");
          }, 300);
        }

        if (text.includes("STREAM STARTED")) {
          console.log("ğŸ™ Stream started");
          audioBuffer = [];
          fs.writeFileSync("session_audio.raw", ""); // Ğ¾Ñ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ñ„Ğ°Ğ¹Ğ»
        }
      });

      esp.on("close", () => {
        console.log("ğŸ”Œ ESP disconnected");
        oa.close();
      });
    } catch (err) {
      console.error("âŒ Setup error:", err.message);
      if (esp.readyState === WebSocket.OPEN) esp.close();
    }
  });
}

start().catch(console.error);
